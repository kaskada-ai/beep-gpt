{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors v1\n",
    "\n",
    "### Goal\n",
    "\n",
    "Let users specify notification topics, for example:\n",
    "\n",
    "\"Hey BeepGPT, let me know when thereâ€™s an important engineering decision being made about the Fraud Detection Project.\"\n",
    "\n",
    "### Method\n",
    "\n",
    "Create embeddings for conversations and store them in a vector store. Then search on each topic for nearest matches alert\n",
    "if conversations come within some threshold.\n",
    "\n",
    "#### Pros:\n",
    "* Fast response. Only requires an LLM to do embeddings of the conversation and search over the vector space\n",
    "* Fairly cheap to run.\n",
    "* Can potentially learn user's specific interests over time from via clustering analysis in the vector space.\n",
    "\n",
    "#### Cons:\n",
    "* Requires a vector database\n",
    "\n",
    "### Questions:\n",
    "* Will the topic embeddings be close to the associated conversations in the vector space?\n",
    "* Are there different embedding methods that will work better than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the tools, initiate the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q kaskada llama-cpp-python ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kaskada as kd\n",
    "\n",
    "# Initialize Kaskada with a local execution context.\n",
    "kd.init_session()\n",
    "\n",
    "# set pandas to display all floats with 6 decimal places\n",
    "pd.options.display.float_format = '{:.6f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull in the user list, create a `format_user()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_json(\"slack-generation.users.json\")\n",
    "\n",
    "columns_to_keep = [\"id\", \"team_id\", \"name\", \"deleted\", \"real_name\", \"is_bot\", \"updated\"]\n",
    "\n",
    "users_df.drop(columns=users_df.columns.difference(columns_to_keep), inplace=True)\n",
    "\n",
    "users = {}\n",
    "for user in users_df.to_dict(orient='index').values():\n",
    "    users[user[\"id\"]] = user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(user_id):\n",
    "    return users[user_id] if user_id in users.keys() else None\n",
    "\n",
    "def format_user(user_id):\n",
    "    user = get_user(user_id)\n",
    "    return f\"{user['name']} ({user_id})\" if user else f\"({user_id})\"\n",
    "\n",
    "format_user(\"UBB9D2B01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the slack data, clean the message text, format message users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events from a Parquet file\n",
    "#\n",
    "# if you wan to load in your own slack data, change this to the path of your output file from 1.1 above\n",
    "# otherwise continue with `slack-generation.parquet`, which contains generated slack data for\n",
    "# example purposes. See the `slack-generation/notebook.ipynb` notebook for more info.\n",
    "input_file = \"slack-generation.parquet\"\n",
    "\n",
    "# Use the \"ts\" column as the time associated with each row,\n",
    "# and the \"channel\" column as the entity associated with each row.\n",
    "raw_msgs = await kd.sources.Parquet.create(\n",
    "    input_file,\n",
    "    time_column = \"ts\",\n",
    "    key_column = \"channel\",\n",
    "    time_unit = \"s\"\n",
    ")\n",
    "raw_msgs.preview(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@kd.udf(\"f<N: any>(x: N) -> string\")\n",
    "def format_users(batch: pd.Series):\n",
    "    # Apply to each row in the batch\n",
    "    return batch.map(format_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text\n",
    "import re\n",
    "\n",
    "def strip_code_blocks(line):\n",
    "    return re.sub(r\"```.*?```\", '', line)\n",
    "\n",
    "def user_repl(match_obj):\n",
    "    user_id = match_obj.group(1)\n",
    "    return format_user(user_id)\n",
    "\n",
    "def update_users(line):\n",
    "    return re.sub(r\"<@(.*?)>\", user_repl, line)\n",
    "\n",
    "def clean_message(text):\n",
    "        text = strip_code_blocks(update_users(text)).strip()\n",
    "        return None if text == \"\" else text\n",
    "\n",
    "@kd.udf(\"f<N: any>(x: N) -> string\")\n",
    "def clean_text(batch: pd.Series):\n",
    "    # Apply to each row in the batch\n",
    "    return batch.map(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_msgs = raw_msgs.extend({\n",
    "    \"text\": raw_msgs.col(\"text\").pipe(clean_text),\n",
    "    \"user\": raw_msgs.col(\"user\").pipe(format_users)\n",
    "})\n",
    "formatted_msgs.preview(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the non-threaded messages into threaded messages. See `FineTuning_v2.ipynb` for more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "ts = formatted_msgs.col(\"ts\")\n",
    "thread_ts = formatted_msgs.col(\"thread_ts\")\n",
    "\n",
    "# split messages into two subgroups: threads and non-threads\n",
    "threads = formatted_msgs.filter(thread_ts.is_not_null())\n",
    "non_threads = formatted_msgs.filter(thread_ts.is_null())\n",
    "\n",
    "# for non-threads, consider a message a new conversation when\n",
    "# more than 10 mins have elapsed since the previous message\n",
    "is_new = ts.seconds_since_previous() > timedelta(minutes=10)\n",
    "\n",
    "# Eventually this will just be: `thread_ts = ts.first(window=kd.windows.Since(is_new, start=\"inclusive\"))`\n",
    "#\n",
    "# However, `Since()` is currently exclusive on the start of the window, inclusive on the end.\n",
    "# But we need inclusive on the start and exclusive on the end.\n",
    "#\n",
    "# The hack below does what we need until `Since()` provides additional options for inclusivity\n",
    "shifted_non_threads = non_threads.shift_by(timedelta(microseconds=0.001))\n",
    "shifted_ts = shifted_non_threads.lag(1).col(\"ts\").first(window=kd.windows.Since(is_new))\n",
    "thread_ts = ts.if_(is_new).else_(shifted_ts)\n",
    "\n",
    "# create threads_ts column for non-threaded messages\n",
    "non_threads_threads = non_threads.extend({\"thread_ts\": thread_ts}).filter(ts.is_not_null().and_(thread_ts.is_not_null()))\n",
    "\n",
    "# re-join the two message subgroups\n",
    "joined = threads.else_(non_threads_threads)\n",
    "\n",
    "# join non-threads and threads back up, and key by conversations\n",
    "messages = joined.with_key(kd.record({\n",
    "        \"channel\": joined.col(\"channel\"),\n",
    "        \"thread\": joined.col(\"thread_ts\"),\n",
    "    }))\n",
    "messages.preview(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect up all the messages, reactions, users in each conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kd.udf(\"f<N: any>(x: N) -> string\")\n",
    "def format_message(batch: pd.Series):\n",
    "    def formatter(raw):\n",
    "        return f\"{raw['user']} --> {raw['text']}\" # --> {raw['reactions']}\"\n",
    "    return batch.map(formatter)\n",
    "\n",
    "@kd.udf(\"f<N: any>(x: N) -> string\")\n",
    "def format_messages(batch: pd.Series):\n",
    "    def formatter(raw):\n",
    "        return \"\\n---\\n\".join(raw)\n",
    "    return batch.map(formatter)\n",
    "\n",
    "@kd.udf(\"f<N: any>(x: N) -> string\")\n",
    "def extract_users(batch: pd.Series):\n",
    "    def get_users(raw):\n",
    "        users = [raw[\"user\"]]\n",
    "        # for user in json.loads(raw[\"reactions\"]).keys():\n",
    "        #     if user not in users:\n",
    "        #         users.append(user)\n",
    "        return json.dumps(users)\n",
    "    return batch.map(get_users)\n",
    "\n",
    "@kd.udf(\"f<N: any>(x: N) -> string\")\n",
    "def unique_users(batch: pd.Series):\n",
    "    def get_users(raw):\n",
    "        users = []\n",
    "        for user_set in raw:\n",
    "            users.extend(json.loads(user_set))\n",
    "        return json.dumps(list(set(users)))\n",
    "    return batch.map(get_users)\n",
    "\n",
    "conversations = kd.record({\n",
    "    # \"conversation\": messages.select(\"user\", \"text\", \"reactions\").pipe(format_message).collect(max=None).pipe(format_messages),\n",
    "    # \"users\": messages.select(\"user\", \"reactions\").pipe(extract_users).collect(max=None).pipe(unique_users),\n",
    "    \"conversation\": messages.select(\"user\", \"text\").pipe(format_message).collect(max=None).pipe(format_messages),\n",
    "    \"users\": messages.select(\"user\").pipe(extract_users).collect(max=None).pipe(unique_users),\n",
    "})\n",
    "conversations.preview(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use LlamaIndex to insert all the conversations into a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create LlamaIndex documents\n",
    "\n",
    "from llama_index import Document\n",
    "\n",
    "documents = []\n",
    "async for row in conversations.run_iter(results=kd.results.Snapshot(), kind=\"row\"):\n",
    "    document = Document(\n",
    "        id= row[\"_key\"],\n",
    "        text=row[\"conversation\"],\n",
    "        metadata={\n",
    "            'users': row[\"users\"],\n",
    "        })\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then insert the documents into the vector store\n",
    "\n",
    "from llama_index import VectorStoreIndex, ServiceContext, set_global_service_context\n",
    "\n",
    "# This will use llama2-chat-13B from with LlamaCPP, and assumes you have llama-cpp-python installed\n",
    "service_context = ServiceContext.from_defaults(llm=\"local\")\n",
    "\n",
    "set_global_service_context(service_context)\n",
    "index = VectorStoreIndex([])\n",
    "for doc in documents:\n",
    "    index.insert(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a LlamaIndex retriever to test topics. \n",
    "\n",
    "This will convert the topic into an embedding and then find the nearest matches in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "# build retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    "    vector_store_query_mode=\"default\",\n",
    "    alpha=None,\n",
    "    doc_ids=None,\n",
    ")\n",
    "\n",
    "nodes = retriever.retrieve(\"Tell me about engineering discussions related to the Supply Chain Management project.\")\n",
    "for node in nodes:\n",
    "    display_source_node(node, source_length=2000, show_source_metadata=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
