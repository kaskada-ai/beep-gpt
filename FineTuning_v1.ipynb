{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235d821b-1ff8-4ef6-8f0b-559c95254479",
   "metadata": {},
   "source": [
    "# BeepGPT Example\n",
    "\n",
    "In this notebook, you’ll see how to train BeepGPT on your Slack history in 15 minutes using only OpenAI’s API’s and open-source Python libraries - Data Science PhD not required.\n",
    "\n",
    "We'll train BeepGPT in four steps:\n",
    "1. Pull down historical messages\n",
    "2. Build training examples\n",
    "3. Convert our examples into a training dataset of prompt/completion pairs\n",
    "4. Send our training data to OpenAI and create a fine-tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70440303",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install timestreams pandas pyarrow openai kaskada==0.6.0a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea2e95-6d9d-4068-ab98-8cf94bc4d9d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import kaskada as kd\n",
    "import pandas\n",
    "import openai\n",
    "import getpass\n",
    "import pyarrow\n",
    "import datetime\n",
    "\n",
    "# Initialize Kaskada with a local execution context.\n",
    "kd.init_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a119b0",
   "metadata": {},
   "source": [
    "## Combine Historical Files\n",
    "\n",
    "Historical slack messages can be exported by following the instructions in Slack's [Export your workspace data](https://slack.com/help/articles/201658943-Export-your-workspace-data) web page. We'll use these messages to teach BeepGPT about the members of your workspace.\n",
    "\n",
    "The export from Slack contains a zip of numererous folders and files. After uncompressing the archive, there are folders for each public channel in your Slack workspace. Inside each folder are json files for each day, which each contain all the events from the day.\n",
    "\n",
    "We execute a short python script (utilizing pandas), to concatenate all the data files together into a single parquet file.\n",
    "\n",
    "Parquet files store data in columns instead of rows. Some benefits of Parquet include:\n",
    "* Fast queries that can fetch specific column values without reading full row data\n",
    "* Highly efficient column-wise compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_file_df(json_path):\n",
    "    df = pd.read_json(json_path, precise_float=True)\n",
    "    # drop rows where subType is not null\n",
    "    if \"subtype\" in df.columns:\n",
    "        df = df[df[\"subtype\"].isnull()]\n",
    "    # only keep these columns\n",
    "    df = df[df.columns.intersection([\"ts\", \"user\", \"text\", \"reactions\", \"thread_ts\"])]\n",
    "    return df\n",
    "\n",
    "def get_channel_df(channel_path):\n",
    "    dfs = []\n",
    "    for root, dirs, files in os.walk(channel_path):\n",
    "        for file in files:\n",
    "            dfs.append(get_file_df(os.path.join(root, file)))\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def get_export_df(export_path):\n",
    "    dfs = []\n",
    "    for root, dirs, files in os.walk(export_path):\n",
    "        for dir in dirs:\n",
    "            df = get_channel_df(os.path.join(root, dir))\n",
    "            # add channel column\n",
    "            df[\"channel\"] = dir\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_slack_export = \"slack-export\"\n",
    "\n",
    "get_export_df(path_to_slack_export).to_parquet(\"messages.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c5682-bfe0-44ca-9a5a-52a0da74e5de",
   "metadata": {},
   "source": [
    "## Read Historical Messages\n",
    "\n",
    "Load messages with Kaskada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d224bec-e5a1-4c67-8764-e3dcdbc5e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events from a Parquet file\n",
    "# Use the \"ts\" column as the time associated with each row, \n",
    "# and the \"channel\" column as the entity associated with each row.\n",
    "messages = await kd.sources.Parquet.create(\n",
    "    path = \"./messages.parquet\", \n",
    "    time_column = \"ts\", \n",
    "    key_column = \"channel\",\n",
    "    time_unit = \"s\",\n",
    ")\n",
    "\n",
    "# View the first 5 events\n",
    "messages.preview(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076d2bf-6830-460b-a9cb-948d8f106edc",
   "metadata": {},
   "source": [
    "## Build examples\n",
    "\n",
    "Fine-tuning examples will teach the model the specific users who are interested in a given conversation. \n",
    "\n",
    "Each example consists of a \"prompt\" containing the state of a conversation at a point in time and a \"completion\" containing the users (if any) who were interested in the conversation. \n",
    "\n",
    "BeepGPT uses several ways to measure interest, for example, replying to a message, or adding an emoji reaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d2a45-eb89-47ce-b471-a39ad8c7bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages come from Slack in chronological order, mixing concurrent conversations together.\n",
    "# Let's re-group messages by thread and/or channel.\n",
    "messages = messages.with_key(kd.record({\n",
    "    \"channel\": messages.col(\"channel\"),\n",
    "    \"thread\": messages.col(\"thread_ts\"),\n",
    "}))\n",
    "\n",
    "# To understand a converstion we need more than the most recent message.\n",
    "# We build a rolling window of messages over the most recent messages.\n",
    "conversations = (\n",
    "    messages\n",
    "    .select(\"user\", \"ts\", \"text\", \"reactions\")\n",
    "    .collect(max=20)\n",
    ")\n",
    "\n",
    "# We want to know who's interested in a conversation, but we only learn that later.\n",
    "# Shift the conversation forward in time so we can associate it with reactions that happen during that time.\n",
    "shifted_conversations = conversations.shift_by(datetime.timedelta(minutes=5))\n",
    "\n",
    "# One signal that someone is interested in a conversation is that they react to it.\n",
    "# Collect all the users who reacted to the conversation in the past 5m\n",
    "# (the period of time the prompt was shifted across)\n",
    "reaction_users = (\n",
    "    messages\n",
    "    .collect(window=kd.windows.Trailing(datetime.timedelta(minutes=5)), max=100)\n",
    "    .col(\"reactions\").flatten()\n",
    "    .col(\"users\").flatten()\n",
    ")\n",
    "\n",
    "# Another signal is that someone responds to the conversation.\n",
    "# Collect all the users to posted messages in the past 5m.\n",
    "participating_users = (\n",
    "    messages\n",
    "    .collect(window=kd.windows.Trailing(datetime.timedelta(minutes=5)), max=100)\n",
    "    .col(\"user\")\n",
    ")\n",
    "\n",
    "# Now we can bring together conversations with the reactions that occurred after the conversation occurred.\n",
    "# We're combining timelines defind at different times here, so we filter the result to the times of the shifted conversations.\n",
    "# This functions similar to a \"left join\".\n",
    "history = (\n",
    "    kd.record({\n",
    "        \"conversation\": shifted_conversations, \n",
    "        \"engaged_users\": reaction_users.union(participating_users),\n",
    "    })\n",
    "    .filter(shifted_conversations.is_not_null())\n",
    ")\n",
    "\n",
    "history.preview(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fa9bd-9c40-403d-a7ee-a15620a88418",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "To prepare our fine-tuning data for OpenAI, we'll use Scikit-Learn for preprocessing. This step ensures that each user is represented by a single \"token\", and that the conversation is formatted in a way that is easy for the model to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Given a conversation, format it for an LLM prompt.\n",
    "# Put each message on a new line, and add a prompt/completion separator at the end of the string.\n",
    "@kd.udf(\"f<N: any>(x: N) -> string\")\n",
    "def format_prompts(batch: pd.Series):\n",
    "    #  Concatenate messages and add a separator\n",
    "    def format_prompt(conversation):\n",
    "        msgs = [msg[\"text\"].strip() for msg in reversed(conversation)]\n",
    "        return \"\\n\\n\".join(msgs) + \" \\n\\n###\\n\\n\"\n",
    "        \n",
    "    # Apply to each row in the batch\n",
    "    return batch.map(format_prompt)\n",
    "\n",
    "# We're going to generate per-token probability estimates, so each user needs to correspond to a single token.\n",
    "# This list will map each user-id string to an integer.\n",
    "labels = [\"nil\"]\n",
    "\n",
    "# Given a list of engaged users, format it for an LLM completion.\n",
    "# NOTE: Predicting a single user rather than a list of them improves model performance\n",
    "@kd.udf(\"g<N: any>(x: N) -> string\")\n",
    "def format_completions(batch: pd.Series):\n",
    "    # Extend labels with any new user ID's in the batch\n",
    "    global labels\n",
    "    for new_label in np.unique(batch.explode().dropna()):\n",
    "        if new_label not in labels:\n",
    "            labels.append(new_label)\n",
    "            \n",
    "    # Randomly pick a single user (if there are multiple), or return \"nil\" if nobody engaged\n",
    "    def format_completion(engaged_users):\n",
    "        if len(engaged_users) > 0:\n",
    "            completion_user = random.choice(engaged_users)\n",
    "            completion_user = str(labels.index(completion_user))\n",
    "        else:\n",
    "            completion_user = \"nil\"\n",
    "        \n",
    "        return \" \" + completion_user + \" end\"\n",
    "    \n",
    "    # Apply to each row in the batch        \n",
    "    return batch.map(format_completion)\n",
    "\n",
    "# Convert our structured data into unstructured training examples.\n",
    "# This requires string manipulations that Kaskada doesn't provide, but we can easily drop down to Python using a UDF.\n",
    "examples = kd.record({\n",
    "    \"prompt\": history.col(\"conversation\").pipe(format_prompts),\n",
    "    \"completion\": history.col(\"engaged_users\").pipe(format_completions),\n",
    "})\n",
    "\n",
    "examples.preview(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86fe98-8a80-45a9-9b28-f357e9d6ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write examples to file\n",
    "examples.to_pandas()[[\"prompt\", \"completion\"]].to_json(\"examples.jsonl\", orient='records', lines=True)\n",
    "print(\"Wrote examples to 'examples.jsonl'\")\n",
    "\n",
    "# Write our user-id : label mapping to file\n",
    "with open('labels.json', 'w') as f:\n",
    "    json.dump(labels, f)\n",
    "print(\"wrote labels to 'labels.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc60311-5ca1-49f3-8e35-4070174e0258",
   "metadata": {},
   "source": [
    "## Fine-tune a custom model\n",
    "\n",
    "Finally, we'll send our fine-tuning examples to OpenAI to create a custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83914ada-d108-422b-b4c0-7a0d9576d031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import cli\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Initialize OpenAI\n",
    "openai.api_key = getpass.getpass('OpenAI: API Key')\n",
    "\n",
    "# Verifiy data format, split for training & validation, upload to OpenAI\n",
    "args = SimpleNamespace(file='./examples.jsonl', quiet=True)\n",
    "cli.FineTune.prepare_data(args)\n",
    "training_id = cli.FineTune._get_or_upload('./examples_prepared_train.jsonl', True)\n",
    "\n",
    "# Train a model using OpenAI's fine-tuning API\n",
    "resp = openai.FineTune.create(\n",
    "    training_file = training_id,\n",
    "    model = \"curie\",\n",
    "    n_epochs = 8,\n",
    "    learning_rate_multiplier = 0.02,\n",
    "    suffix = \"coversation_users\"\n",
    ")\n",
    "\n",
    "# Fine-tuning can take awhile, so keep track of this ID\n",
    "print(f'Fine-tuning model with job ID: \"{resp[\"id\"]}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
