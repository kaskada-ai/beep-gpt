{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235d821b-1ff8-4ef6-8f0b-559c95254479",
   "metadata": {},
   "source": [
    "# BeepGPT Example\n",
    "\n",
    "In this notebook, you’ll see how to train BeepGPT on your Slack history in 15 minutes using only OpenAI’s API’s and open-source Python libraries - Data Science PhD not required.\n",
    "\n",
    "We'll train BeepGPT in four steps:\n",
    "1. Pull down historical messages\n",
    "2. Build training examples\n",
    "3. Convert our examples into a training dataset of prompt/completion pairs\n",
    "4. Send our training data to OpenAI and create a fine-tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70440303",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas pyarrow openai kaskada==0.6.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea2e95-6d9d-4068-ab98-8cf94bc4d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import kaskada as kt\n",
    "import pandas\n",
    "import openai\n",
    "import getpass\n",
    "import pyarrow\n",
    "import datetime\n",
    "\n",
    "# Initialize Kaskada with a local execution context.\n",
    "kt.init_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c5682-bfe0-44ca-9a5a-52a0da74e5de",
   "metadata": {},
   "source": [
    "## Read Historical Messages\n",
    "\n",
    "Historical slack messages can be exported by following the instructions in Slack's [Export your workspace data](https://slack.com/help/articles/201658943-Export-your-workspace-data) web page. We'll use these messages to teach BeepGPT about the members of your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d224bec-e5a1-4c67-8764-e3dcdbc5e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events from a Parquet file\n",
    "# Use the \"ts\" column as the time associated with each row, \n",
    "# and the \"channel\" column as the entity associated with each row.\n",
    "messages = kt.sources.Parquet(\n",
    "    path = \"./messages.parquet\", \n",
    "    time_column_name = \"ts\", \n",
    "    key_column_name = \"channel\",\n",
    ")\n",
    "\n",
    "# View the first 5 events\n",
    "messages.preview(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076d2bf-6830-460b-a9cb-948d8f106edc",
   "metadata": {},
   "source": [
    "## Build examples\n",
    "\n",
    "Fine-tuning examples will teach the model the specific users who are interested in a given conversation. Each example consists of a \"prompt\" containing the state of a conversation at a point in time and a \"completion\" containing the users (if any) who were interested in the conversation. BeepGPT uses several ways to measure interest, for example, replying to a message, or adding an emoji reaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d2a45-eb89-47ce-b471-a39ad8c7bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-group messages by thread and/or channel\n",
    "# Slack messages are delivered chronologically, so messages in threads\n",
    "# may be interleaved with messages in the main channel.\n",
    "messages = messages.with_key(kt.record({\n",
    "        \"channel\": messages.col(\"channel\"),\n",
    "        \"thread\": messages.col(\"thread_ts\"),\n",
    "    }))\n",
    "\n",
    "# Build the GPT input prompt by collecting relevant fields of recent messages\n",
    "conversations = (\n",
    "    messages\n",
    "    .select(\"user\", \"ts\", \"text\", \"reactions\")\n",
    "    .collect(max=20)\n",
    ")\n",
    "\n",
    "\n",
    "# Shift the prompt forward in time 5m to observe the effects of the conversation\n",
    "shifted_conversations = conversations.shift_by(datetime.timedelta(seconds=1))\n",
    "\n",
    "# Collect all the users who reacted to the conversation in the past 5m\n",
    "# (the period of time the prompt was shifted across)\n",
    "reaction_users = (\n",
    "    messages\n",
    "    .collect(window=kt.windows.Trailing(datetime.timedelta(minutes=5)), max=100)\n",
    "    .col(\"reactions\").flatten()\n",
    "    .col(\"users\").flatten()\n",
    ")\n",
    "\n",
    "# Collect all the users to posted messages in the past 5m\n",
    "participating_users = (\n",
    "    messages\n",
    "    .collect(window=kt.windows.Trailing(datetime.timedelta(minutes=5)), max=100)\n",
    "    .col(\"user\")\n",
    ")\n",
    "\n",
    "# Build a fine-tuning example mapping a conversation to the users who reacted to it\n",
    "history = (\n",
    "    kt.record({\n",
    "        \"conversation\": shifted_conversations, \n",
    "        \"engaged_users\": reaction_users.union(participating_users),\n",
    "    })\n",
    "    .filter(shifted_conversations.is_not_null())\n",
    ")\n",
    "\n",
    "history.preview(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fa9bd-9c40-403d-a7ee-a15620a88418",
   "metadata": {},
   "source": [
    "## Create training dataset\n",
    "\n",
    "To prepare our fine-tuning data for OpenAI, we'll use Scikit-Learn for preprocessing. This step ensures that each user is represented by a single \"token\", and that the conversation is formatted in a way that is easy for the model to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy, json\n",
    "\n",
    "# Extract examples from historical data\n",
    "history_df = history.run().to_pandas().drop([\"_time\", \"_subsort\", \"_key_hash\", \"_key\"], axis=1)\n",
    "\n",
    "\n",
    "# Encode user ID labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(history_df.engaged_users.explode())\n",
    "with open('labels_.json', 'w') as f:\n",
    "    json.dump(le.classes_.tolist(), f)\n",
    "\n",
    "\n",
    "# Format for the OpenAI API\n",
    "def format_prompt(conversation):\n",
    "    return \"start -> \" + \"\\n\\n\".join([f' {msg[\"user\"]} --> {msg[\"text\"]} ' for msg in conversation]) + \"\\n\\n###\\n\\n\"\n",
    "def format_completion(engaged_users):\n",
    "    return \" \" + (\" \".join(le.transform(engaged_users).astype(str)) if len(engaged_users) > 0 else \"nil\") + \" end\"\n",
    "    \n",
    "examples_df = pandas.DataFrame({\n",
    "    \"prompt\": history_df.conversation.apply(format_prompt),\n",
    "    \"completion\": history_df.engaged_users.apply(format_completion),\n",
    "})\n",
    "\n",
    "# Write examples to file\n",
    "examples_df.to_json(\"examples.jsonl\", orient='records', lines=True)\n",
    "print(\"Wrote examples to 'examples.jsonl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc60311-5ca1-49f3-8e35-4070174e0258",
   "metadata": {},
   "source": [
    "## Fine-tune a custom model\n",
    "\n",
    "Finally, we'll send our fine-tuning examples to OpenAI to create a custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83914ada-d108-422b-b4c0-7a0d9576d031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import cli\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Initialize OpenAI\n",
    "openai.api_key = getpass.getpass('OpenAI: API Key')\n",
    "\n",
    "# Verifiy data format, split for training & validation, upload to OpenAI\n",
    "args = SimpleNamespace(file='./examples.jsonl', quiet=True)\n",
    "cli.FineTune.prepare_data(args)\n",
    "training_id = cli.FineTune._get_or_upload('./examples_prepared_train.jsonl', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002771e-2569-4fae-8215-01d5f96502f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model using \"davinci\", the most advanced model available for fine-tuning\n",
    "resp = openai.FineTune.create(\n",
    "    training_file = training_id,\n",
    "    model = \"davinci\",\n",
    "    n_epochs = 8,\n",
    "    learning_rate_multiplier = 0.02,\n",
    "    suffix = \"coversation_users\"\n",
    ")\n",
    "\n",
    "# Fine-tuning can take awhile, so keep track of this ID\n",
    "print(f'Fine-tuning model with job ID: \"{resp[\"id\"]}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
