{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235d821b-1ff8-4ef6-8f0b-559c95254479",
   "metadata": {},
   "source": [
    "# Slack Export Generation\n",
    "\n",
    "In this notebook, we will use ChatGTP to generate a slack export.\n",
    "\n",
    "We will generate 2 weeks of Slack chat amongst 6 users of a team. The team will work on 4 different projects over the 2 weeks, and discuss various topics related to each project. All content will be generated.\n",
    "\n",
    "Steps: \n",
    "0. Pre-requisites\n",
    "1. Project Idea & Discussion Topic Generation\n",
    "2. Work Schedule Generation\n",
    "3. Chat Generation\n",
    "4. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d550080",
   "metadata": {},
   "source": [
    "### 0. Pre-requisites\n",
    "\n",
    "First lets install the libraries we will use and initialize our OpenAI session with an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70440303",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pandas openai kaskada==0.6.0a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96fab0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import getpass\n",
    "\n",
    "# Initialize OpenAI\n",
    "openai.api_key = getpass.getpass('OpenAI: API Key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f25883",
   "metadata": {},
   "source": [
    "### 1. Project Idea & Discussion Topic Generation\n",
    "\n",
    "I asked ChatGPT \"What are some example projects that streaming technology would be helpful in?\" and got back 12 ideas, including a longer text describing each idea. \n",
    "\n",
    "I chose 4 of the projects for my slack data generation and built a json blob containing the ideas and descriptions.\n",
    "\n",
    "I then asked ChatGPT: \n",
    "  ```\n",
    "  Can you fill out the following json blob? \n",
    "\n",
    "  for \"discussion_topics\", I'd like you to provide a list of discussion topics that would be helpful for a team developing backend software that works on the current sector, project, description.\n",
    "\n",
    "  for \"technologies\", I'd like you to provide a list of technologies that would be helpful for a team developing backend software that works on the current sector, project, description.\n",
    "\n",
    "  [\n",
    "      {\n",
    "        \"sector\": \"Supply Chain Management\", \"project\": \"Inventory Tracking Tool\",\n",
    "        \"description\": \"Implement streaming to track inventory and shipments in real time, ensuring accurate inventory levels and timely deliveries. Be able to predict when inventories will run low in order to replenish supplies as needed to avoid shutdowns.\",\n",
    "        \"discussion_topics\": [], \"technologies\": []\n",
    "      },\n",
    "      {\n",
    "        \"sector\": \"E-Commerce\", \"project\": \"Personalized Product Recommendations\",\n",
    "        \"description\": \"Utilize streaming to analyze user behavior on an e-commerce website in real time, providing personalized product recommendations based on browsing and purchase history.\",\n",
    "        \"discussion_topics\": [], \"technologies\": []\n",
    "      },\n",
    "      {\n",
    "        \"sector\": \"Financial Services\", \"project\": \"Real-Time Fraud Detection\",\n",
    "        \"description\": \"Use streaming to process and analyze financial transactions in real time, identifying potential fraudulent activities and triggering alerts immediately.\",\n",
    "        \"discussion_topics\": [], \"technologies\": []\n",
    "      },\n",
    "      {\n",
    "          \"sector\": \"Telecommunications\", \"project\": \"Network Monitoring\",\n",
    "          \"description\": \"Employ streaming to process network data in real time, identifying network issues, outages, or abnormal patterns for quick resolution\",\n",
    "        \"discussion_topics\": [], \"technologies\": []\n",
    "        }\n",
    "    ]\n",
    "  ```\n",
    "\n",
    "The results of this request are contained in `projects.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c682369",
   "metadata": {},
   "source": [
    "### 2. Work Schedule Generation\n",
    "\n",
    "The script below iterates over the `projects.json` file and creates a work schedule from the content.\n",
    "\n",
    "Our team will work M-F, 8am to 5pm. For each hour, we will figure out what is the current project, what is the next project, and what is the current discussion topic.\n",
    "\n",
    "The output of this script is `schedule.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78ad3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar, datetime, json\n",
    "from datetime import timedelta\n",
    "\n",
    "now = datetime.datetime.fromisoformat(\"2023-07-31 00:00:00\")\n",
    "end = datetime.datetime.fromisoformat(\"2023-08-11 17:00:00\")\n",
    "\n",
    "projects = {}\n",
    "with open(\"projects.json\") as in_file:\n",
    "    projects = json.load(in_file)\n",
    "\n",
    "\n",
    "current = 0\n",
    "\n",
    "def project_text(project):\n",
    "    if project >= 0 and project < 4:\n",
    "        return f'Sector: {projects[project][\"sector\"]}, Name: {projects[project][\"project\"]}, Description: {projects[project][\"description\"]}'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "with open(f'schedule.jsonl', 'w') as out_file:\n",
    "    while now < end:\n",
    "        now = now.__add__(timedelta(hours=1))\n",
    "\n",
    "        # only work 8-5 m-f\n",
    "        if now.weekday() > 4 or now.hour < 8 or now.hour >= 17:\n",
    "            continue\n",
    "\n",
    "        # get next topic\n",
    "        if current >= 0 and current < 4:\n",
    "            discussions = len(projects[current][\"discussion_topics\"])\n",
    "            technologies = len(projects[current][\"technologies\"])\n",
    "            if discussions == 0 and technologies == 0:\n",
    "                current += 1\n",
    "        \n",
    "        if current >= 0 and current < 4:\n",
    "            discussions = len(projects[current][\"discussion_topics\"])\n",
    "            technologies = len(projects[current][\"technologies\"])\n",
    "\n",
    "            if discussions > 0 and discussions >= technologies:\n",
    "                topic = f'The primary discussion topic for this hour is: \"{projects[current][\"discussion_topics\"].pop()}\"'\n",
    "            else:\n",
    "                topic = f'The primary technology to discuss in this hour is: \"{projects[current][\"technologies\"].pop()}\"'\n",
    "        else:\n",
    "            topic = \"\"\n",
    "\n",
    "        obj = {\n",
    "            \"datetime\": f'{calendar.day_name[now.weekday()]} {now.strftime(\"%m/%d/%Y, %H:%M:%S\")}',\n",
    "            \"timestamp\": int(datetime.datetime.timestamp(now)),\n",
    "            \"topic\": topic,\n",
    "            \"current_project\": project_text(current),\n",
    "            \"next_project\": project_text(current + 1),\n",
    "        }\n",
    "        out_file.write(json.dumps(obj) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1ad7f",
   "metadata": {},
   "source": [
    "### 3. Chat Generation\n",
    "\n",
    "* We will now use the openAI ChatCompletion API to generate our slack conversations. We do this by providing a large set of instructions to the model.\n",
    "* Most of the instructions are constant, but some of them change on a per-hour request, based on our work schedule.\n",
    "* We also send the previous instructions and response, so that the model can continue conversations where it previously left off.\n",
    "* Even though we ask for more than 100 messages on each return, we rarely get more than 50. You can experiment with changing the instructions to try to increase the message response count.\n",
    "* We use `gpt-3.5-turbo-16k` as the model because of its high token-limit\n",
    "  * Note: `gpt4` generally did a worse job of following the instructions, returned fewer & shorter messages, and is more expensive to use.\n",
    "* Each response from the model will take 2-3 minutes to return. The total time to run the whole script is about 5 hours. The total cost is about $3 USD.\n",
    "  * The script is written to append data, so you restart it without loosing any history\n",
    "\n",
    "The output is saved to `generated.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "190bfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this only once, or if you want to completely restart generation\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "instructions = \"\"\"\n",
    "I'd like you to simulate a long-running slack conversation between 6 team members. The team works from 8am to 5pm GMT. The team works together developing backend software using streaming technologies.\n",
    "There will be 2 channels: \"General\" and \"Project\".\n",
    "In the \"General\" channel, team members will talk about their weekends, plan team meetings, and occasionally discuss new technologies.\n",
    "In the \"Project\" channel, team members will discuss their current project. About 80% of the total simulation should occur in this channel. \n",
    "The project they are working on will change over time. They will mostly discuss the current project, but will occasionally discuss ideas for the next project.\n",
    "Each user has a different role and experience level.\n",
    "UserA is a senior engineer with expert level knowledge of Kafka & Java, beginner with Pulsar & Python.\n",
    "UserB is a junior engineer with mostly Python experience, in school they had a minor in data-science.\n",
    "UserC is the manager of the team, mid-senior level, knows about streaming concepts, but doesn't know any programming language well.\n",
    "UserD is the PM of the team, used to be a developer, has a strong python background in data-science, but has no streaming technology experience.\n",
    "UserE is senior level developer with strong Python, no-sql database experience, and despite being senior sometimes it takes them a long time to grasp new concepts. However, after they grasp the concept, they are very good at helping other engineers with it.\n",
    "UserF is a principal level engineer with strong knowledge of everything related to the projects. \n",
    "Some topics can be discussed in threads. Each thread should have a unique integer id. If no one has spoken on a thread for more than 20 minutes, either a new thread should be started, or the discussion can continue in the channel (outside a thread).\n",
    "The output format of each message should be a single json line, and include: the message timestamp, the channel, the userId speaking, the threadId (or 'null' if not in a thread), and then the text.\n",
    "An example output for a message outside a thread: Example: {\"timestamp\": 1653465675, \"channel\": \"General\", \"user\": \"UserA\", \"thread\": null, \"text\": \"i had a great weekend, did you?\"}\n",
    "An example output for a message inside a thread: Example: {\"timestamp\": 1653292875, \"channel\": \"Project\", \"user\": \"UserA\", \"thread\": 123, \"text\": \"thanks for your help explaining that concept\"}\n",
    "When a discussion is on-going the team members should reply back within one minute. There should be at least 10 minute breaks between topics or discussions.\n",
    "\"\"\"\n",
    "\n",
    "schedule = {}\n",
    "prompt = None\n",
    "response = None\n",
    "res = None\n",
    "total_usage = 0\n",
    "start_time = time.time()\n",
    "last_line = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07edb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to stop and restart this as often as you like until it completes\n",
    "\n",
    "current_line = 0\n",
    "with open(\"generated.jsonl\", \"a\") as out_file:\n",
    "    with open(\"schedule.jsonl\") as in_file:\n",
    "        for line in in_file.readlines():\n",
    "            current_line +=1\n",
    "\n",
    "            if current_line < last_line:\n",
    "                continue\n",
    "\n",
    "            schedule = json.loads(line)\n",
    "\n",
    "            if res:\n",
    "                total_usage += res[\"usage\"][\"total_tokens\"]\n",
    "                print(f'Previous call usage: {res[\"usage\"]}')\n",
    "                print(f'Total usage: {total_usage} tokens.')\n",
    "                elapsed_time = time.time() - start_time\n",
    "                # Convert elapsed_time to hours, minutes, and seconds\n",
    "                hours = int(elapsed_time // 3600)\n",
    "                elapsed_time %= 3600\n",
    "                minutes = int(elapsed_time // 60)\n",
    "                seconds = int(elapsed_time % 60)\n",
    "                \n",
    "                # Print the elapsed time in hours, minutes, and seconds\n",
    "                print(f'Elapsed time: {hours:02}:{minutes:02}:{seconds:02}')\n",
    "                print()\n",
    "\n",
    "            msgs = [{\"role\": \"system\", \"content\": instructions}]\n",
    "\n",
    "            if prompt and response:\n",
    "                msgs.append(prompt)\n",
    "                msgs.append(response)\n",
    "\n",
    "            print(f'Building prompt for: {schedule[\"datetime\"]} with topic: {schedule[\"topic\"]}:')\n",
    "        \n",
    "            s = f' The current project is: \"{schedule[\"current_project\"]}\"' + \\\n",
    "                f' The next project is: \"{schedule[\"next_project\"]}\" ' + \\\n",
    "                schedule[\"topic\"] + \\\n",
    "                f'. Please generate 100 or more messages for the next hour, starting at: {schedule[\"timestamp\"]}. ' + \\\n",
    "                f' Create at least one thread. The next threadId is: {current_line}. Threads should be 5 to 10 messages long, but don\\'t have to have every user' + \\\n",
    "                f' At least 80% of messages should be in the \"Project\" channel.'\n",
    "\n",
    "            prompt = {\"role\": \"user\", \"content\": s}\n",
    "            msgs.append(prompt)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "                    \n",
    "            res = openai.ChatCompletion.create(\n",
    "                    model = \"gpt-3.5-turbo-16k\",\n",
    "                    messages = msgs\n",
    "                )\n",
    "\n",
    "            response = res[\"choices\"][0][\"message\"]\n",
    "            out_file.write(response[\"content\"] + \"\\n\")\n",
    "            out_file.flush()\n",
    "\n",
    "            last_line = current_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e208a",
   "metadata": {},
   "source": [
    "### 4. Output\n",
    "\n",
    "We will use Kaskada and Pandas to convert the data to the right output format, and then save it to parquet for input into Kaskada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e845fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only once\n",
    "import kaskada as kd\n",
    "kd.init_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = await kd.sources.JsonlFile.create(\n",
    "    \"generated.jsonl\",\n",
    "    time_column = \"timestamp\", \n",
    "    key_column = \"channel\",\n",
    "    time_unit = \"s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8396001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# re-key using thread and channel\n",
    "output = messages.with_key(kd.record({\n",
    "    \"channel\": messages.col(\"channel\"),\n",
    "    \"thread\": messages.col(\"thread\"),\n",
    "}))\n",
    "\n",
    "# on per-entity basis, add thread_ts based off first ts in entity\n",
    "# but only where thread is not null\n",
    "output = output.extend({\"thread_ts\": output.col(\"timestamp\").first().if_(output.col(\"thread\").is_not_null())})\n",
    "\n",
    "# convert to pandas data-frame, drop columns we don't need\n",
    "df=output.to_pandas()\n",
    "df.drop(columns=[\"_time\", \"_key\", \"thread\"], inplace=True)\n",
    "\n",
    "# Rename the 'OldName' column to 'NewName'\n",
    "df.rename(columns={\"timestamp\": \"ts\"}, inplace=True)\n",
    "\n",
    "# Convert the ts column to float64\n",
    "df[\"ts\"] = df[\"ts\"].astype(\"float64\")\n",
    "\n",
    "\n",
    "# generate userIds for users\n",
    "def get_user_id(user):\n",
    "    md5_hash = hashlib.md5()\n",
    "    md5_hash.update(user.encode('utf-8'))\n",
    "    return md5_hash.hexdigest()[:8]\n",
    "\n",
    "df[\"user\"] = df[\"user\"].apply(get_user_id)\n",
    "\n",
    "# write the output to jsonl\n",
    "df.to_json(\"messages.jsonl\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
